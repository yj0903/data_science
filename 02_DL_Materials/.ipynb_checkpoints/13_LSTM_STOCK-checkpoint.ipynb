{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b298f512",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bae439",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915aa18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/AMZN.csv', index_col = 'Date', parse_dates=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8758871",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MinMaxScaler 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78565831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "raw_data = deepcopy(df.drop(['Name'], axis=1))\n",
    "X_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_raw_data = X_scaler.fit_transform(raw_data)\n",
    "y_raw_data = y_scaler.fit_transform(raw_data['Close'].values.reshape(-1, 1))\n",
    "print (X_raw_data.shape, y_raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a786122",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### X_train 과 y_train 데이터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda6e34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_xy(X_raw_data, y_raw_data, lookback):\n",
    "    data = list()\n",
    "    for index in range(len(X_raw_data) - lookback): \n",
    "        data.append(X_raw_data[index: index + lookback])\n",
    "    data = np.array(data)\n",
    "    return data, y_raw_data[lookback:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f0909",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lookback = 10 # sequence length - 1\n",
    "X_train, y_train = prepare_xy(X_raw_data, y_raw_data, lookback)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043e185",
   "metadata": {},
   "source": [
    "### Custom Dataset\n",
    "- MNIST 데이터는 이미 잘 만들어진 데이터로, 바로 Subset, DataLoader 등 파이토치의 미니배치 구성 관련 기능을 사용할 수 있음\n",
    "- raw data 를 기반으로, 파이토치의 미니배치등을 구성하려면, 위와 같이 직접 데이터셋을 작성하여, TensorDataset 을 통해, Dataset 으로 만들면 됨\n",
    "- 이 기법 외에 Custom Dataset 을 구성할 수도 있음\n",
    "\n",
    "> raw data 를 텐서로 만들어, 전체 데이터를 딥러닝 모델에 한번에 넣는 경우는 없으므로, 미니배치를 쉽게 구성하고, 반복문을 통해, 미니배치 하나씩 가져오도록 하는 구성이 필요함\n",
    "> 이를 손쉽게 해주는 기능이 Subset, DataLoader 등의 기능이며, 이를 위해, raw data 를 Dataset 으로 만들어주어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b29a68",
   "metadata": {},
   "source": [
    "### Custom Dataset class\n",
    "- Custom Dataset 은 클래스로 구현해야 함 (모델 구현과 유사함)\n",
    "- torch.utils.data.Dataset 클래스를 상속받아야 함\n",
    "- 다음 세가지 메서드가 필요함\n",
    "  - \\_\\_init\\_\\_(self) : 입력 데이터(x)와 실제 값(y) 을 선언해주는 메서드\n",
    "  - \\_\\_len\\_\\_(self) :입력 데이터(x)와 실제 값(y) 길이를 리턴해주는 메서드\n",
    "  - \\_\\_getitem\\_\\_(self, index) : index번째 입력 데이터(x)와 실제 값(y) 을 리턴해주는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.X_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.X_data)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, index): \n",
    "        X = self.X_data[index]\n",
    "        y = self.y_data[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rawdata = CustomDataset()\n",
    "train_rawdata.y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c35cbd",
   "metadata": {},
   "source": [
    "### TensorDataset\n",
    "- TensorDataset은 Dataset을 상속한 클래스로 학습 데이터 X와 실제 값 Y를 묶어 놓는 Dataset\n",
    "   - 학습 데이터와 실제 값을 하나로 묶어서, 인덱스, 반복문을 통한 각 데이터 추출을 편리하게 하는 기능\n",
    "   - DataLoader 등 pytorch 의 데이터 전처리 기능 사용 가능\n",
    "- TensorDataset으로 랩핑한 Dataset 은 DataLoader 로 미니배치를 쉽게 작성할 수 있음\n",
    "- tensors() 메서드로 각 텐서를 인덱스 번호로 엑세스도 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e06c63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_rawdata = TensorDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f090f0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print (train_rawdata.tensors[0].shape) # X_train\n",
    "print (train_rawdata.tensors[1].shape) # y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56104f",
   "metadata": {},
   "source": [
    "### Generate Train & Validation Mini-batch\n",
    "- TensorDataset 으로 만든 Dataset 이든, Custom Dataset class 로 만든 Dataset 이든 모두 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee129e98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VALIDATION_RATE = 0.2\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train_rawdata)), # X index 번호\n",
    "    test_size=VALIDATION_RATE # test dataset 비율\n",
    ")\n",
    "train_dataset = Subset(train_rawdata, train_indices)\n",
    "validation_dataset = Subset(train_rawdata, val_indices)\n",
    "\n",
    "minibatch_size = 128 # Mini-batch 사이즈는 128 로 설정\n",
    "# create batches\n",
    "train_batches = DataLoader(train_dataset, batch_size=minibatch_size, shuffle=True)\n",
    "val_batches = DataLoader(validation_dataset, batch_size=minibatch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 하나만 가져와서 이미지 visualization \n",
    "X_train, y_train = next(iter(train_batches))\n",
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f38f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61db83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, dropout_p, output_size, model_type='LSTM'):\n",
    "        super().__init__()\n",
    "        if model_type == 'LSTM':\n",
    "            self.sequenceclassifier = nn.LSTM(\n",
    "                input_size = feature_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = dropout_p\n",
    "            )\n",
    "        elif model_type == 'GRU':\n",
    "            self.sequenceclassifier = nn.GRU(\n",
    "                input_size = feature_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = dropout_p\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.sequenceclassifier(x) # |output| = (128, 10, 32)\n",
    "        output = output[:, -1, :] # |output| = (128, 32)\n",
    "        y = self.fc(output)  \n",
    "        return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4f84b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### input, output, loss, optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846b6a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_size = 5 # 입력 차원\n",
    "hidden_size = 32 # Hidden Layer 사이즈 설정처럼 설정\n",
    "num_layers = 2 # stacked RNN (최대 4개까지는 Gradient Vanishing 현상이 적을 수 있으므로)\n",
    "dropout_p = 0 # dropout rate\n",
    "output_size = 1\n",
    "\n",
    "model = Net(feature_size, hidden_size, num_layers, dropout_p, output_size, 'GRU')\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3076809",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### training 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634706b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, early_stop, n_epochs, progress_interval):\n",
    "    \n",
    "    train_losses, valid_losses, lowest_loss = list(), list(), np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_loss, valid_loss = 0, 0\n",
    "        \n",
    "        # train the model\n",
    "        model.train() # prep model for training\n",
    "        for x_minibatch, y_minibatch in train_batches:\n",
    "            y_minibatch_pred = model(x_minibatch)\n",
    "            loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_batches)\n",
    "        train_losses.append(train_loss)      \n",
    "        \n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_minibatch, y_minibatch in val_batches:\n",
    "                y_minibatch_pred = model(x_minibatch)\n",
    "                loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "        valid_loss = valid_loss / len(val_batches)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        if valid_losses[-1] < lowest_loss:\n",
    "            lowest_loss = valid_losses[-1]\n",
    "            lowest_epoch = epoch\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            if (early_stop > 0) and lowest_epoch + early_stop < epoch:\n",
    "                print (\"Early Stopped\", epoch, \"epochs\")\n",
    "                break\n",
    "                \n",
    "        if (epoch % progress_interval) == 0:\n",
    "            print (train_losses[-1], valid_losses[-1], lowest_loss, lowest_epoch, epoch)\n",
    "            \n",
    "    model.load_state_dict(best_model)        \n",
    "    return model, lowest_loss, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bff45",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 훈련 실행\n",
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">본 코드는 CPU 만으로도 테스트하기 때문에, colab 과 GPU 기반으로 실행할 필요는 없습니다. 따라서, 본 코드 기반, 각자 PC 상에서 테스트해보셔도 좋을 것 같습니다</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9a2f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_epochs = 100 \n",
    "progress_interval = 3\n",
    "early_stop = 30\n",
    "\n",
    "model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, nb_epochs, progress_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fffe2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 테스트\n",
    "- torch.cat(합칠텐서리스트, 합치는 차원)\n",
    "   - 합칠텐서 리스트는 기본적으로는 튜플 형태로 넣어야 하지만, 리스트도 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (torch.cat([torch.randn(24, 128, 1), torch.randn(24, 128, 1), torch.randn(24, 128, 1)], 0).shape)\n",
    "print (torch.cat([torch.randn(24, 128, 1), torch.randn(24, 128, 1), torch.randn(24, 128, 1)], 1).shape)\n",
    "print (torch.cat([torch.randn(24, 128, 1), torch.randn(24, 128, 1), torch.randn(24, 128, 1)], 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f484c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_batches = DataLoader(train_rawdata, batch_size=minibatch_size, shuffle=False)\n",
    "y_test_pred_list, y_test_list = list(), list()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_minibatch, y_minibatch in test_batches:\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        y_test_pred_list.append(y_minibatch_pred)\n",
    "        y_test_list.append(y_minibatch)\n",
    "y_test_preds = torch.cat(y_test_pred_list, 0)\n",
    "y_tests = torch.cat(y_test_list, 0)\n",
    "print (y_test_preds.shape, y_tests.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac583981",
   "metadata": {},
   "source": [
    "### 데이터 스케일 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ebcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(y_scaler.inverse_transform(np.array(y_test_preds)))\n",
    "original = pd.DataFrame(y_scaler.inverse_transform(np.array(y_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753804a",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "RMSE = mean_squared_error(original[0], predict[0])**0.5\n",
    "print (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1baf2f3",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo # jupyter notebook 에서 보여지도록 설정하는 부분 (가끔 안나올 때, 이 명령을 하면 됨)\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=predict.index, y=predict[0], name=\"LSTM 예측\", line=dict(color='royalblue', width=1)))\n",
    "fig.add_trace(go.Scatter(x=original.index, y=original[0], name=\"실제 주가\", line=dict(color='firebrick', width=1)))\n",
    "fig.update_layout(\n",
    "    {\n",
    "        \"title\": { \"text\": \"딥러닝 주가 예측 (Amazon)\", \"x\": 0.5, \"y\": 0.9, \"font\": { \"size\": 15 } },\n",
    "        \"showlegend\": True,\n",
    "        \"xaxis\": { \"title\": \"time index\" },\n",
    "        \"yaxis\": { \"title\": \"price\" },\n",
    "        \"template\":\"ggplot2\"\n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b8dbb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
