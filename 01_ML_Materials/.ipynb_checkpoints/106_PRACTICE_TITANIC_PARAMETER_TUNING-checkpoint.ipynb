{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('titanic_step4_importance_train.pickle', 'rb') as pickle_filename:\n",
    "    train_importance = pickle.load(pickle_filename)\n",
    "with open('titanic_step4_importance_test.pickle', 'rb') as pickle_filename:\n",
    "    test_importance = pickle.load(pickle_filename)\n",
    "with open('titanic_step4_importance_train_y.pickle', 'rb') as pickle_filename:\n",
    "    train_answer = pickle.load(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # 각 모델에서 내부적으로 관련 라이브러리 사용 가능\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier             # 1. K-Nearest Neighbor(KNN)\n",
    "from sklearn.linear_model import LogisticRegression            # 2. Logistic Regression\n",
    "from sklearn.svm import SVC                                                # 3. SVC\n",
    "from sklearn.tree import DecisionTreeClassifier                   # 4. Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier       # 5. Random Forest\n",
    "from sklearn.ensemble import ExtraTreesClassifier             # 6. Extra Tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # 7. GBM\n",
    "from sklearn.naive_bayes import GaussianNB                     # 8. GaussianNB\n",
    "from xgboost import XGBClassifier                                     # 9. XGBoost\n",
    "from lightgbm import LGBMClassifier                                 # 10. LightGBM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 튜닝과 SVC\n",
    "- SVC 는 결국 분류의 경계가 되는 경계선을 작성하여, 분류를 실행하는 모델\n",
    "- 해당 경계선을 일직선으로 할지, 어느 정도 곡률을 가진 선으로 할지도 선정 가능\n",
    "- 주요 하이퍼 파라미터\n",
    "  - C : regularization 파라미터\n",
    "  - gamma: 어느 정도 훈련 셋에 fit 하게 할지를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV + SVC\n",
    "- RandomizedSearchCV() 는 랜덤하게 파라미터값을 선정하여, 테스트를 수행하므로,\n",
    "- 보다 적합한 파라미터값을 도출하기 위해서는 수행횟수를 늘려야 함\n",
    "- 따라서 머신러닝 모델의 수행성능에 따라, RandomizedSearchCV() 사용시, 수행시간이 상당히 오래 걸릴 수 있으므로,\n",
    "- RandomizedSearchCV() 이해를 위해서만 일부 모델에서만 테스트를 진행하고,\n",
    "- GridSearchCV() 을 주로 사용하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고: 균일 분포 또는 균등 분포(Uniform Distribution)\n",
    "  - 정해진 범위에서 모든 확률이 균일한 분포를 의미함\n",
    "  - 균일 분포는 이산형 확률 분포와 연속형 확률 분포 두 형태가 존재 \n",
    "  - 연속형 확률 분포: 두 점 a,b 사이의 연속적인 값에 대한 확률 분포\n",
    "  - 이산형 확률 분포: 두 점 a,b 사이에 갯수가 정해진 값들에 대한 확률 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats.uniform(loc, scale)\n",
    "- loc 부터, loc + scale 까지의 범위에서 균등한 확률로 연속형 값을 추출\n",
    "- 해당 객체는 rvs() 메서드를 가지고 있고, 이를 사용해서, RandomizedSearchCV() 가 랜덤 값을 균등 확률로 추출해서, 적용 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"C\": stats.uniform(0, 50),\n",
    "    \"gamma\": stats.uniform(0, 1)\n",
    "}\n",
    "gd = RandomizedSearchCV(\n",
    "    estimator = SVC(random_state=1), \n",
    "    param_distributions=hyperparams, \n",
    "    n_iter=100, \n",
    "    cv=5,   # 내부적으로 (Stratified)KFold 사용\n",
    "    scoring='accuracy', \n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "#  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "# gd.fit(train_importance, train_answer) \n",
    "gd.fit(train_importance, train_answer.values.ravel())\n",
    "\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)\n",
    "\n",
    "df = pd.DataFrame(gd.cv_results_)\n",
    "print(df[['params','mean_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "> 하이퍼파라미터는 일반적으로 적절한 범위가 없기 때문에, 각 데이터에 맞춰서 성능이 나오는 범위를 감으로 지정해야 함\n",
    "> RandomizedSearchCV() 를 통해, 대략적인 범위를 알아낸 후, 이를 기반으로 GridSearchCV() 를 사용하여\n",
    "> 범위와, 최적의 값을 가질 수 있는 후보군을 지정하는 방식으로, 최적의 하이퍼파라미터 값을 찾아가는 방법을 많이 사용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 그리드 셋팅\n",
    "hyperparams = {\n",
    "    'C': [10, 15, 20, 23, 25, 30, 50], \n",
    "    'gamma' : [0.001, 0.01, 0.05, 0.06, 0.07, 0.1]\n",
    "}\n",
    "\n",
    "# 교차검증\n",
    "gd=GridSearchCV(\n",
    "    estimator = SVC(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5,   # 내부적으로 (Stratified)KFold 사용\n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "#  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "# gd.fit(train_importance, train_answer) \n",
    "gd.fit(train_importance, train_answer.values.ravel())\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다, PC에 따라 다르겠지만, 다소 예측 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting Classifier 주요 하이퍼 파라미터\n",
    "- learning_rate는 학습률을 의미하며, 각 트리의 오류에 기반해서, 어느 정도 수정할지의 비율을 의미\n",
    "- n_estimator 는 트리의 갯수를 의미\n",
    "- max_depth 는 트리의 깊이를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "- Gradient Boosting Classifier 는 수행시간이 오래 걸리므로, RandomizedSearchCV() 로 사전 테스트를 통해, 대략적인 최적의 파라미터값을 예상하여, GridSearchCV() 로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = [0.01, 0.05, 0.1, 0.2]\n",
    "n_estimators = [100, 1000, 2000]\n",
    "max_depth = [3, 5, 10, 15]\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = GradientBoostingClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "#  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "# gd.fit(train_importance, train_answer) \n",
    "gd.fit(train_importance, train_answer.values.ravel())\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다, 성능이 개선됨</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic Regression 주요 하이퍼 파라미터\n",
    "* penalty: regularization 종류 선정 (l1, l2 등)\n",
    "* C: regularization 적용 강도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + 하이퍼 파라미터 튜닝 (RandomizedSearchCV 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본 코드는 컴퓨터 성능에 따라 수행시간이 매우 오래 걸리고, 수행시간 제한으로 주피터 노트북등이 다운될 수도 있음\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "hyperparams = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'], \n",
    "    'C': stats.uniform(0, 1000)\n",
    "}\n",
    "gd = RandomizedSearchCV(\n",
    "    # 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "    #  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "    # estimator = LogisticRegression(random_state=1)\n",
    "    estimator = LogisticRegression(random_state=1, solver='lbfgs', max_iter=1000), \n",
    "    param_distributions=hyperparams, \n",
    "    n_iter=100, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "#  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "# gd.fit(train_importance, train_answer) \n",
    "gd.fit(train_importance, train_answer.values.ravel())\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "- numpy.linspace(start, end, num)\n",
    "  - start ~ end 사이의 값을 등간격으로 num 갯수만큼의 배열을 생성하는 numpy 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(700, 900, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = np.linspace(700, 900, 200)\n",
    "\n",
    "hyperparams = {\n",
    "    'penalty': penalty, \n",
    "    'C': C\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    # 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "    #  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "    # estimator = LogisticRegression(random_state=1)\n",
    "    estimator = LogisticRegression(random_state=1, solver='lbfgs', max_iter=1000), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "# 2022.12.22 해당 라이브러리 변경으로 인터페이스가 변경되어,  기존 코드로 경고표시가 나옵니다.\n",
    "#  경고표시가 나오더라도, 진행에는 문제가 없지만, 당황하실 수 있어서, 경고표시가 나오지 않도록 코드를 업데이트합니다.\n",
    "# gd.fit(train_importance, train_answer) \n",
    "gd.fit(train_importance, train_answer.values.ravel())\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있으며,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다는 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. XGBoost 와 주요 하이퍼 파라미터\n",
    "- 일반적인 XGBoost 하이퍼 파라미터 튜닝 전략\n",
    "   - ensemble 방식은 수행시, 각 파라미터를 적절히 맞춰주기 때문에, 하이퍼 파라미터 튜닝이 정확도를 높이는데 있어서, 큰 기여를 하는 편은 아니며, \n",
    "   - 무수히 많은 파라미터가 있고, 수행 시간이 오래 걸리므로, 주요한 파라미터들만 중심으로 튜닝을 진행하는 편이 좋음\n",
    "- 성능에 영향을 많이 끼치는 주요 파라미터\n",
    "   - learning_rate \n",
    "      - 이전 결과를 얼마나 반영할지에 대한 학습 단계별 적용할 가중치를 의미함 \n",
    "      - 일반적으로 0.01 ~ 0.2 사이의 값을 많이 사용함\n",
    "   - max_depth\n",
    "      - 트리의 최대 깊이를 의미함\n",
    "      - 트리의 최대 깊이로 -1 로 하면, 깊이에 제한을 두지 않음 \n",
    "      - 일반적으로 3 ~ 10 사이의 값을 많이 사용함\n",
    "   - gamma\n",
    "      - 일종의 정규화(regularization) 파라미터로, gamma 가 높을 수록, regularization 이 높다고 이해하면 됨\n",
    "      - 트리에서 가지를 추가로 만들기 위해 필요한 최소 loss 기준값으로, gamma 값이 작으면, 트리에 보다 많은 가지가 만들어진다고 이해하면 됨\n",
    "      - 일반적으로 0 이상의 값을 가짐\n",
    "   - min_child_weight\n",
    "      - 트리에서 가지를 추가로 만들어 분할하기 위해, 필요한 최소한의 샘플 수\n",
    "      - 값이 적을 수록, 트리가 더 분할될 수 있음\n",
    "      - 일반적으로 0 이상의 값을 가짐\n",
    "   - subsample\n",
    "      - 각 트리마다 모든 훈련데이터를 사용해서 트리를 만들지 않음\n",
    "      - 훈련 데이터의 일부를 사용해서 트리를 만든다면, 보다 많은 트리를 만들 수 있고, 이를 통해 트리의 다양성을 높일 수 있음\n",
    "      - subsample 은 이 때, 각 트리마다 어느 정도의 훈련 데이터 비율을 사용해서 트리를 만들지를 그 비율을 정하는 것임\n",
    "      - 일반적으로 0.5 ~ 1 사이의 값을 많이 사용함\n",
    "   - colsample_bytree\n",
    "      - subsample 과 마찬가지로, 훈련 데이터에서 일부 feature (컬럼) 들만 뽑아서 트리를 만드는 방식\n",
    "      - 일부 feature (컬럼)들만 뽑아서 트리를 만든다면, 보다 많은 트리를 만들 수 있고, 이를 통해 트리의 다양성을 높일 수 있음\n",
    "      - colsample_bytree 는 이를 위해 각 트리를 만들 때 사용할 feature의 비율을 정하는 것임\n",
    "      - 일반적으로 0.5 ~ 1 사이의 값을 많이 사용함\n",
    "      \n",
    "   - reg_alpha: L1 정규화(regularization) 가중치\n",
    "   - reg_lambda: L2 정규화(regularization) 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization 를 위한 파이썬 라이브러리 설치\n",
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">2022.12.22 업데이트</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">라이브러리가 업데이트되었으나, 버그가 있어서, 에러가 발생하고 있습니다.</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">따라서, 라이브러리 버전을 기존 버전으로 고정하였습니다</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization XGBoost 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pbounds = {  \n",
    "    'learning_rate': (0.01, 0.5),  \n",
    "    'n_estimators': (100, 1000), \n",
    "    'max_depth': (3, 10),\n",
    "    'min_child_weight': (0, 10),\n",
    "    'subsample': (0.5, 1.0),  \n",
    "    'colsample_bytree': (0.5, 1.0),   \n",
    "    'gamma': (0, 5)\n",
    "    # 'reg_lambda': (0, 1000, 'log-uniform'),\n",
    "    # 'reg_alpha': (0, 1.0, 'log-uniform')    \n",
    "}\n",
    "\n",
    "def xgboost_hyper_param(learning_rate, n_estimators, max_depth, min_child_weight, subsample, colsample_bytree, gamma):\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth, \n",
    "        min_child_weight= min_child_weight,\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        subsample=subsample, \n",
    "        colsample_bytree=colsample_bytree, \n",
    "        gamma=gamma,\n",
    "        random_state=1,\n",
    "        eval_metric='logloss'\n",
    "        # reg_alpha=reg_alpha,\n",
    "        # reg_lambda=reg_lambda\n",
    "    )\n",
    "    return np.mean(cross_val_score(clf, train_importance, train_answer, cv=5, scoring='accuracy'))\n",
    "\n",
    "optimizer = BayesianOptimization( f=xgboost_hyper_param, pbounds=pbounds, random_state=1)\n",
    "# init_points: 초기 랜덤 포인트 갯수\n",
    "# acq='ei': Expected Improvement \n",
    "# xi=0.01: exploration(불확실성이 가장 높은 점 근처에 최적값이 존재할 것이라는 가정으로 계산된 값) 강도 (보통 0.01) \n",
    "optimizer.maximize(init_points=10, n_iter=100, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전, XGBoost 는 약 82.26% 였지만, 튜닝 후, 84.51% 까지 예측 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization LightGBM 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pbounds = {  \n",
    "    'learning_rate': (0.01, 0.5),  \n",
    "    'n_estimators': (100, 1000), \n",
    "    'max_depth': (3, 10),\n",
    "    'min_child_weight': (0, 10),    \n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0)\n",
    "    # 'reg_lambda': (0, 1000),\n",
    "    # 'reg_alpha': (0, 1.0)\n",
    "}\n",
    "\n",
    "def lgbm_hyper_param(learning_rate, n_estimators, max_depth, min_child_weight, subsample, colsample_bytree):\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = LGBMClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        subsample=subsample, \n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=1\n",
    "        # reg_lambda=reg_lambda,        \n",
    "        # reg_alpha=reg_alpha\n",
    "    )\n",
    "    return np.mean(cross_val_score(clf, train_importance, train_answer, cv=5, scoring='accuracy'))   # cv 도 숫자로 작성하여, 내부적으로 (Stratified)KFold 사용함\n",
    "\n",
    "optimizer = BayesianOptimization( f=lgbm_hyper_param, pbounds=pbounds, verbose=1, random_state=1)\n",
    "optimizer.maximize(init_points=10, n_iter=100, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">LightGBM 은 수행시간은 XGBoost 보다 단축되고, </font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">예측 정확도는 거의 유사하거나, 살짝 낮은 정도임을 확인할 수 있으므로, 많은 테스트를 위해서는 LightGBM을 사용하는 것도 좋음</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Grid Search XGBoost 적용\n",
    "\n",
    "### Grid Search XGBoost 적용 1단계\n",
    "- 주요 파라미터를 모두 넣을 경우, 수행시간이 매우 길어지므로, 3단계로 나누어서 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.001, 0.005, 0.01, 0.05, 0.06, 0.1, 0.12, 0.15, 0.17, 0.2]\n",
    "n_estimators = [10, 50, 60, 75, 85, 100, 125, 150, 200, 250, 500, 1000]\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(random_state=1, eval_metric='logloss'), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search XGBoost 적용 2단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "min_child_weight = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "hyperparams = {\n",
    "    'max_depth': max_depth, \n",
    "    'min_child_weight': min_child_weight\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(learning_rate=0.17, n_estimators=10, random_state=1, eval_metric='logloss'), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search XGBoost 적용 3단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma =  [i*0.1 for i in range(0,5)]\n",
    "subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "colsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n",
    "\n",
    "hyperparams = {\n",
    "    'gamma': gamma,\n",
    "    'subsample':subsample,\n",
    "    'colsample_bytree':colsample_bytree,\n",
    "    'reg_alpha': reg_alpha\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(\n",
    "        learning_rate=0.17, \n",
    "        n_estimators=10, \n",
    "        max_depth=6, \n",
    "        min_child_weight=1,\n",
    "        random_state=1,\n",
    "        eval_metric='logloss'\n",
    "    ), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">XGBoost 를 Grid Search 에 적용할 때, </font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">사전에 여러 테스트를 통해, 파라미터값에 대한 대략적인 감을 가진다면, 보다 좋은 성능 값을 찾아낼 수도 있음</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "hyperparams = { \n",
    "    'n_neighbors': n_neighbors\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = KNeighborsClassifier(), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [3, None] \n",
    "max_features = [0.1, 0.2, 0.5, 0.8, 'sqrt', 'log2'] # feature 수\n",
    "min_samples_split = [2, 4, 6, 8, 10] # 노드를 분할하기 위한 최소 샘플 수\n",
    "min_samples_leaf = [2, 4, 6, 8, 10] # 리프 노드가 되기 위해 필요한 최소 샘플 수\n",
    "\n",
    "hyperparams = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth, \n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split, \n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = RandomForestClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [3, None] \n",
    "max_features = [0.1, 0.2, 0.5, 0.8, 'sqrt', 'log2'] # feature 수\n",
    "min_samples_split = [2, 4, 6, 8, 10] # 노드를 분할하기 위한 최소 샘플 수\n",
    "min_samples_leaf = [2, 4, 6, 8, 10] # 리프 노드가 되기 위해 필요한 최소 샘플 수\n",
    "\n",
    "hyperparams = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth, \n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split, \n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = ExtraTreesClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">훈련 셋을 랜덤하게 분리하므로, 예측 성능은 조금씩 차이는 있을 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">하이퍼파라미터 튜닝 후, 개선된 성능을 확인할 수 있음</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
