{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2668fa5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9505fc6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "제 유투브 채널로 본 강의를 포함한 데이터 전과정 로드맵과 관련 커리어를 이해할 수 있는 영상도 참고로 확인해보시면<br> \n",
    "학습하시는데 큰 그림을 이해하실 수 있으실꺼예요. (괜찮으시면 구독과 좋아요도 부탁드립니다. ㅎ)<br>\n",
    "<b>- 데이터, AI 커리어와 데이터 전과정 학습 방법 완벽 가이드: <a href=\"https://youtu.be/vsoAyh4D-zw\">https://youtu.be/vsoAyh4D-zw</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228ac6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 텐서간 곱셈 정리와 관련 메서드 익히기\n",
    "\n",
    "> 딥러닝 기본기를 위해 꼭 정리가 필요한 부분이 텐서간 곱셈과 관련 메서드 사용법 정리입니다. <br>\n",
    "> 텐서간 곱셈은 선형대수 과목에서 익히지만, 해당 과목을 익히지 않았다면, 이 부분이 큰 장벽이 될 수 있습니다. <br>\n",
    "> 따라서, 딥러닝에 꼭 필요한 부분에 대해서 정리하기로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f7f22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PyTorch 와 텐서간의 곱셈\n",
    "- torch.matmul() 메서드가 다양한 텐서곱셈을 지원하므로, 상세히 이해할 필요가 있음\n",
    "- 이외에 torch.mm() 은 2D 텐서, torch.bmm() 은 3D 텐서 간의 연산만 지원하며, matmul() 과의 차이점도 알아둘 필요가 있음\n",
    "\n",
    "> matmul() 은 텐서의 shape 등에 따라, 다양한 계산이 가능하고, broadcasting 도 지원하므로, 자칫 예상치 못한 연산이 될 수도 있음<br>\n",
    "> 디버깅을 위해, 기대한 케이스에 대해서만 명확한 계산을 하는 것이 필요하다면, torch.mm(), torch.bmm() 사용을 고려할 필요도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8211f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1D 텐서(벡터) X 1D 텐서(벡터) 의 곱셈\n",
    "  - 1D 텐서(벡터) 끼리의 곱셈은 벡터의 내적값, 즉 스칼라(scala) 값을 리턴함\n",
    "  - 두 벡터는 동일 차원이어야 함\n",
    "  - 벡터의 내적은 선형대수에서 나오는 수식으로, dot product 라고도 불리움\n",
    "  - 간단히 다음과 같이 공식만 정리하면 됨\n",
    "\n",
    "     - $ \\vec{A} = (a_1, a_2, a_3), \\vec{B} = (b_1, b_2, b_3) $ 이라고 한다면, $A$ 와 $B$ 벡터의 내적은,\n",
    "       - $ \\vec{AB} = a_1b_1 + a_2b_2 + a_3b_3 $ 임\n",
    "     - 이를 행렬식으로 표시한다면, 다음과 같음 ( $\\vec{A} = (2, 2, 2), \\vec{B} = (3, 3, 3) $ 일 경우를 예를 들면,\n",
    "       - $ [2, 2, 2] \\cdot \\begin{bmatrix} 3 \\\\ 3  \\\\ 3  \\end{bmatrix} = 2 \\times 3 + 2 \\times 3 + 2 \\times 3 = 18 $     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e84f9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2]) 1 torch.Size([3])\n",
      "tensor([3, 3, 3]) 1 torch.Size([3])\n",
      "tensor(18) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.full((3, ), 2) # vector 생성\n",
    "B = torch.full((3, ), 3) # vector 생성\n",
    "print (A, A.dim(), A.shape)\n",
    "print (B, B.dim(), B.shape)\n",
    "result = torch.matmul(A, B)\n",
    "print (result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ed68b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2D 텐서(행렬) X 2D 텐서(행렬)  곱\n",
    "- Matrix Multiplication, Inner Product, 또는 Dot Product 라고 부름\n",
    "- 앞 행렬의 열의 갯수와 뒷 행렬의 행의 갯수가 같아야 행렬간 곱셈이 가능\n",
    "  - $ A = \\begin{bmatrix} a & b \\\\ c & d \\\\ e & f \\end{bmatrix}, x ∈ \\mathbb{R}^{3 \\times 2}$ \n",
    "  - $ B = \\begin{bmatrix} g & h & i  \\\\ j & k & l \\end{bmatrix}, x ∈ \\mathbb{R}^{2 \\times 3}$ \n",
    "  - $ AB = \\begin{bmatrix} a & b \\\\ c & d \\\\ e & f \\end{bmatrix} * \\begin{bmatrix} g & h & i  \\\\ j & k & l \\end{bmatrix} = \\begin{bmatrix} ag + bj & ah + bk & ai + bl  \\\\ cg + dj & ch + dk & ci + dl \\\\ eg + ej & eh + fk & ei + fl \\end{bmatrix} $\n",
    "\n",
    "  <img src=\"https://www.fun-coding.org/00_Images/matrix-multiple.png\">\n",
    "- $  x ∈ \\mathbb{R}^{3 \\times 2} $ 은 torch.shape 으로 표기시, (3, 2) 과 같이 표현됨\n",
    "- torch.shape 를 기준으로 행렬 곱을 통한 tensor 의 shape 를 정리하면, 다음과 같음\n",
    "\n",
    "  <img src=\"https://www.fun-coding.org/00_Images/matrix-multiple-summary.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926aebac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.full((3, 2), 2)\n",
    "B = torch.full((2, 3), 3) # vector 생성\n",
    "result = torch.matmul(A, B)\n",
    "print (result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44507a42",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1D 텐서(벡터) X 2D 텐서(행렬) 의 곱 \n",
    "- 벡터(vector) x 행렬(matrix) 와 행렬(matrix) x 벡터(vector) 는 계산식이 다름\n",
    "- 벡터는 기본적으로 열 벡터로 다룸\n",
    "- 따라서, 다음과 같이 표시됨\n",
    "  - $ x = \\begin{bmatrix} 2 \\\\ 2  \\\\ 2 \\end{bmatrix}, x ∈ \\mathbb{R}^3, (3, ) \\rightarrow (3) $\n",
    "- $A$ 가 다음 행렬일 때, \n",
    "  - $ A = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{bmatrix}, A ∈ \\mathbb{R}^{3 \\times 2}, (3, 2)$ \n",
    "- $xA$ 는\n",
    "  - $ xA = \\begin{bmatrix} 2 \\\\ 2  \\\\2 \\end{bmatrix} \\times \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{bmatrix} $\n",
    "- $ x = (3), A = (3, 2) $ 를 torch.matmul() 로 계산하면, 다음과 같이 $ xA = (2) $ 가 됨\n",
    "- 이는 벡터(vector) x 행렬(matrix) 일 때에는,\n",
    "  - $ x^T = [ 2 \\ 2 \\ 2 ] = (1, 3) $ 으로 transpose 를 한 후,\n",
    "    - $ x^T \\times A $ 가 되므로, $ [ 2 \\ 2 \\ 2 ] \\times \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\times 3 + 2 \\times 3 + 2 \\times 3 & 2 \\times 3 + 2 \\times 3 + 2 \\times 3 \\end{bmatrix}  $\n",
    "    - $ x^T \\times A = (2) $ 가 됨\n",
    "\n",
    " > 벡터(vector) x 행렬(matrix) 은 벡터의 차원 수를 (a),  행렬의 열의 수 (b, c) 일 때, a 와, b 가 같아야 하므로, <br>\n",
    " > $ (a) \\times (a, b) $ 가 되며, 결과 shape 는 a 가 삭제된 (b) 가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628ee979",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 18]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.full((3, ), 2)\n",
    "A = torch.full((3, 2), 3)\n",
    "result = torch.matmul(x, A)\n",
    "print (result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6796bac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1D 텐서(벡터) X 3D 이상 텐서 의 곱\n",
    "- 1D 텐서(벡터) 를 (a) 라 하고,\n",
    "- 3D 이상 텐서 를 (b, c, d) 라 하면,\n",
    "   - 3D 이상 텐서는 batched matrix 로 간주하여, b x (c, d) 가 됨\n",
    "- 즉, b x ((a) x (c, d)) 가 되므로, 1D 텐서 X 2D 텐서 와 마찬가지로, a 와 c 는 동일해야 함\n",
    "- 즉, (a) x (b, a, d) 가 되고, 결과 shape 는 동일한 a 는 삭제되고, (b, d) 가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b29c6939",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[18, 18],\n",
      "         [18, 18],\n",
      "         [18, 18],\n",
      "         [18, 18]],\n",
      "\n",
      "        [[18, 18],\n",
      "         [18, 18],\n",
      "         [18, 18],\n",
      "         [18, 18]]]) torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.full((3, ), 2)\n",
    "A = torch.full((4, 3, 2), 3)\n",
    "result = torch.matmul(x, A)\n",
    "print (result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b0657",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2D 텐서(행렬) X 1D 텐서(벡터) 의 곱 \n",
    "  - $ x = \\begin{bmatrix} 2 \\\\ 2  \\end{bmatrix}, x ∈ \\mathbb{R}^2, (2, ) \\rightarrow (2) $\n",
    "- $A$ 가 다음 행렬일 때, \n",
    "  - $ A = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{bmatrix}, A ∈ \\mathbb{R}^{3 \\times 2}, (3, 2)$ \n",
    "- $Ax$ 는\n",
    "   - $ Ax = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{bmatrix} \\times \\begin{bmatrix} 2 \\\\ 2  \\end{bmatrix} $\n",
    "   - $ Ax = \\begin{bmatrix} 3 \\times 2 + 3 \\times 2 & 3 \\times 2 + 3 \\times 2 & 3 \\times 2 + 3 \\times 2  \\end{bmatrix} $\n",
    " \n",
    " > 행렬(matrix) x 벡터(vector) 는 행렬의 열의 수 (a, b) 일 때, b 와, 벡터의 차원 수 (c) 라고 할 때, c 가 같아야 하므로, <br>\n",
    " > $ (a, b) \\times (b) $ 가 되며, 결과 shape 는 b 가 삭제된 (a) 가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306190a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 12, 12]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((3, 2), 3)\n",
    "result = torch.matmul(A, x)\n",
    "print (result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beca73",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3D 이상 텐서 X 1D 텐서(벡터) 의 곱 \n",
    "- 3D 이상 텐서 를 (a, b, c) 라 하고,\n",
    "   - 3D 이상 텐서는 batched matrix 로 간주하여, a x (b, c) 가 됨\n",
    "- 1D 텐서(벡터) 를 (d) 라 하면,\n",
    "\n",
    "- 즉, a x ((b, c) x (d)) 가 되므로, 2D 텐서 X 1D 텐서 와 마찬가지로, c 와 d 는 동일해야 함\n",
    "- 즉, (a, b, c) x (c) 가 되고, 결과 shape 는 동일한 c 는 삭제되고, (a, b) 가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0478a431",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]]) torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((4, 3, 2), 3)\n",
    "result = torch.matmul(A, x)\n",
    "print (result, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5604f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $n$-D 텐서 (n > 2) 간의 곱셈\n",
    "\n",
    "- 기본적으로는 첫번째 인자와 두번째 인자의 마지막 두 axis 차원 이외의 동일함을 가정하고, Batched Matrix Multiplication 방식으로 계산됨\n",
    "  - Batched Matrix Multiplication 방식 이해\n",
    "    - (b, m, n) 은 (m, n) 행렬이 b 개 있는 것이라고 볼 수 있음\n",
    "    - (b, m, n) x (b, n, k) 는 b 개의 (m, n) 을 b 개의 (n, k) 와 곱하는 것이라고 볼 수 있음\n",
    "    - 따라서, tensor 간의 곱에서 b 자리는 동일해야 하며, (m, n), (n, k) 는 행렬 곱셈과 동일한 제약사항을 가짐\n",
    "  \n",
    "<img src=\"https://www.fun-coding.org/00_Images/bmm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bc2e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 첫번째 인자와 두번째 인자의 마지막 두 axis 차원 이외의 차원이 동일하지 않다면, broadcasting 룰이 적용됨\n",
    "  - 첫번째 인자의 마지막 두 axis 차원, 즉 matrix (m, n) 과, 두번째 인자의 마지막 두 axis 차원, 즉 matrix (n, k) 사이에서만 곱하여 (m, k) 으로 계산함\n",
    "    - 이 때 두 인자의 마지막 두 axis 차원 이외의 앞 부분은 broadcast 룰이 적용됨\n",
    "       - 예: 첫번째 인자 (a, b, c, d), 두번째 인자 (a, b, d, e) 또는 첫번째 인자 (a, b, c, d), 두번째 인자 (d, e)\n",
    "\n",
    "<img src=\"https://www.fun-coding.org/00_Images/matmul_new.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2dae012",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 torch.Size([10, 1, 3, 4])\n",
      "3 torch.Size([2, 4, 5])\n",
      "torch.Size([10, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.full((10, 1, 3, 4), 2) # matrix 생성\n",
    "data2 = torch.full((2, 4, 5), 3) # vector 생성\n",
    "\n",
    "print (data1.dim(), data1.shape) # 10 x (3, 4)\n",
    "print (data2.dim(), data2.shape) # (4, 5)\n",
    "data3 = torch.matmul(data1, data2) # 10 x ((3, 4) x (4, 5))\n",
    "print (data3.shape) # 10 x (3, 5) = (10, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bee577",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 torch.Size([10, 8, 3, 4])\n",
      "4 torch.Size([1, 8, 4, 5])\n",
      "torch.Size([10, 8, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.full((10, 8, 3, 4), 2) # matrix 생성\n",
    "data2 = torch.full((1, 8, 4, 5), 3) # vector 생성\n",
    "\n",
    "print (data1.dim(), data1.shape) # 10 x (3, 4)\n",
    "print (data2.dim(), data2.shape) # (4, 5)\n",
    "data3 = torch.matmul(data1, data2) # 10 x ((3, 4) x (4, 5))\n",
    "print (data3.shape) # 10 x (3, 5) = (10, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028fc4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### torch.mm()\n",
    "- 행렬곱만 지원함\n",
    "- 두 인자 모두 행렬 shape 이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e468acb0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([3, 4])\n",
      "2 torch.Size([4, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.full((3, 4), 2) # matrix 생성\n",
    "data2 = torch.full((4, 5), 3) # vector 생성\n",
    "\n",
    "print (data1.dim(), data1.shape) # (3, 4)\n",
    "print (data2.dim(), data2.shape) # (4, 5)\n",
    "data3 = torch.mm(data1, data2)\n",
    "print (data3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93572b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### torch.bmm()\n",
    "- 두 인자 모두 3D 텐서 shape 이어야 함\n",
    "- broadcast 를 지원하지 않으므로, 마지막 두 axis 외에는 두 인자 모두 동일한 차원이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49b8a96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 torch.Size([3, 3, 4])\n",
      "3 torch.Size([3, 4, 5])\n",
      "torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.full((3, 3, 4), 2) # matrix 생성\n",
    "data2 = torch.full((3, 4, 5), 3) # vector 생성\n",
    "\n",
    "print (data1.dim(), data1.shape) # (3, 4)\n",
    "print (data2.dim(), data2.shape) # (4, 5)\n",
    "data3 = torch.bmm(data1, data2)\n",
    "print (data3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d756812",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
