{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fd4ff2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b99758",
   "metadata": {},
   "source": [
    "### Pytorch 모델 용어 정리\n",
    "- 계층(layer): 모듈 또는 모듈을 구성하는 한 개의 계층을 의미함 (예: 선형 계층, Linear Layer)\n",
    "- 모듈(module): 한 개 이상의 계층이 모여 구성된 것. 모듈이 모여서 새로운 모듈 구성 가능\n",
    "- 모델(model): 최종적인 네트워크. 한 개의 모듈이 모델이 될 수도 있고, 여러 개의 모듈이 하나의 모델이 될 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fce78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### torch.nn 과 nn.Module\n",
    "- torch.nn 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소 제공\n",
    "- 모든 PyTorch 모듈은 nn.Module 의 하위 클래스(subclass) 임\n",
    "- 모든 PyTorch 신경망 모델은 nn.Module 을 상속받은 하위 클래스로 정의함\n",
    "   - \\_\\_init\\_\\_ 에서 신경망 계층 초기화 필요\n",
    "   - forward() 메서드에서 입력 데이터에 대한 연산 정의 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835e0bf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Raw-level Linear Layer 구현\n",
    "- Linear Layer 이론, 수학적 이해, 실제 PyTorch 의 필요 문법까지 익혔으므로,\n",
    "- 이를 기반으로 Linear Layer 의 보다 선명한 이해 및 PyTorch 에 익숙해지기 위해, Linear Layer 를 가볍게 구현해보기로 함\n",
    "- 입력 차원이 4, 출력 차원이 3 이고, Linear Layer 함수를 $ f(x) = x * W + b $ 이라고 하면,\n",
    "- 다음과 같은 방식으로 계산되므로, \n",
    "<img src=\"https://www.fun-coding.org/00_Images/linearlayer-matrix.png\">\n",
    "- $ W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23}  \\\\ w_{31} & w_{32} & w_{33}  \\\\ w_{41} & w_{42} & w_{43} \\end{bmatrix} , W ∈ \\mathbb{R}^{4×3}, |W| =(4, 3) $, 즉 $ W = $ (입력차원, 출력차원) 이 됨\n",
    "\n",
    "  - b 는 벡터이므로, 열 벡터로 표현하면, $ b = \\begin{bmatrix} b_1 \\\\ b_2  \\\\ b_3  \\end{bmatrix} , b ∈ \\mathbb{R}^{3}, |b| =(3, ) $\n",
    "  - x 도 벡터이므로, 열 벡터로 표현하면, $ x = \\begin{bmatrix} x_1 \\\\ x_2  \\\\ x_3 \\\\ x_4 \\end{bmatrix}, x ∈ \\mathbb{R}^{4}, |x| =(4, )$\n",
    "  - 즉 $ |x| =(4), |W| =(4, 3), |b| =(3) $ 임\n",
    "  - Linear Layer 함수는 $ f(x) = x * W + b $ 이므로, vector X matrix + vector 가 됨\n",
    "  - 벡터(vector) x 행렬(matrix) 은 벡터의 차원 수를 (a),  행렬의 열의 수 (b, c) 일 때, a 와, b 가 같아야 하고, 이 때 결과 shape 는 (a) x (a, c) = (c) 가 됨\n",
    "  - (4) x (4, 3) = (3) 이 되고, (3) + (3) = (3) 이 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3186376c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.FloatTensor(4) # 입력\n",
    "W = torch.FloatTensor(4, 3) # 가중치\n",
    "b = torch.FloatTensor(3) # 편향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4eb8bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Linear Layer 함수를 정의한다면,\n",
    "def linearfunction(x, W, b):\n",
    "        y = torch.matmul(x, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d5c91b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W torch.Size([4, 3]) x torch.Size([4]) b torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print (\"W\", W.shape, \"x\", x.shape, \"b\", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bb338",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = linearfunction(x, W, b)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f0619",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Module 기반, Linear Layer 구현\n",
    "- 신경망 모델 클래스를 만들고, nn.Module 을 상속받음\n",
    "- \\_\\_init\\_\\_ 에서 신경망 계층 초기화 선언\n",
    "- forward() 메서드에서 입력 데이터에 대한 연산 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1567c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super() 함수는 super(subclass 이름, subclass 객체).__init__()  와 같이 써야 하지만,\n",
    "        # 하부클래스 선언 내부에서 super() 호출 시는 super().__init__() 와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "        super().__init__()\n",
    "        # __init__() 에서 신경망 계층 초기화\n",
    "        self.W = torch.FloatTensor(4, 3)\n",
    "        self.b = torch.FloatTensor(3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim)\n",
    "        # |y| = (input_dim) * (input_dim, output_dim) + (output_dim)\n",
    "        #      = (output_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75647988",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork()\n",
    "# forward 에 넣을 인자값으로 호출하면, 내부적으로 forward() 메서드를 자동 호출함 (정석적 방법)\n",
    "# 내부 처리 중, forward() 전처리/후처리도 수행하므로, forward() 를 직접 호출하면, 전처리/후처리를 수행하지 않게될 수 있음\n",
    "y = mylinear(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dc38a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print (y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf50e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # super() 함수는 super(subclass 이름, subclass 객체).__init__()  와 같이 써야 하지만,\n",
    "        # 하부클래스 선언 내부에서 super() 호출 시는 super().__init__() 와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "        super().__init__()\n",
    "        # __init__() 에서 신경망 계층 초기화\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim)\n",
    "        # |y| = (input_dim) * (input_dim, output_dim) + (output_dim)\n",
    "        #      = (output_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bc5bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork(4, 3)\n",
    "y = mylinear(x)\n",
    "print (y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e6753",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for param in mylinear.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13626bb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Module 기반, nn.Parameter 등록하기\n",
    "- 학습 대상이 되는 텐서는 해당 모듈에 연결된 Parameter 로 등록해야 함\n",
    "   - 이를 통해 특정 모듈에서 학습 처리시 필요한 작업을 알아서 해주도록 구성되어 있음\n",
    "   - 모듈 내에서 학습 대상이 되는 텐서들은 \\_\\_init\\_\\_ 에서 nn.Parameter() 으로 등록해줘야 함\n",
    "      - nn.Parameter(텐서, requires_grad=True)\n",
    "         - requires_grad 는 디폴트 True 이며, 파라미터가 gradient(점진적으로 최적값을 찾아가는 방식) 방식으로 계산되는 케이스를 의미함\n",
    "         - 내부적으로 gradient 방식에서 필요한 미분 연산을 위해, 파이토치는 동적으로 그래프를 구성하므로, 이런 구성도 필요한 파라미터임을 의미함\n",
    "- 기본 신경망 모델 코드 작성 방법\n",
    "   1. 신경망 모델 클래스를 만들고, nn.Module 을 상속받음\n",
    "   2. \\_\\_init\\_\\_ 에서 신경망 계층 초기화 선언 (모듈 내에서 학습 대상이 되는 텐서들은 nn.Parameter() 으로 등록)\n",
    "   3. forward() 메서드에서 입력 데이터에 대한 연산 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03de1e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # super() 함수는 super(subclass 이름, subclass 객체).__init__()  와 같이 써야 하지만,\n",
    "        # 하부클래스 선언 내부에서 super() 호출 시는 super().__init__() 와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "        super().__init__()\n",
    "        # __init__() 에서 신경망 계층 초기화\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim)\n",
    "        # |y| = (input_dim) * (input_dim, output_dim) + (output_dim)\n",
    "        #      = (output_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ca57a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork(4, 3)\n",
    "y = mylinear(x)\n",
    "print (y, y.shape)\n",
    "for param in mylinear.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afc23f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Linear 클래스\n",
    "- 지금까지는 Linear Layer 와 PyTorch 기반 신경망 모듈 구현 방법을 이해하기 위해, Linear Layer 를 PyTorch 의 신경망 모듈 클래스로 구현해본 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9bd37ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6823, 0.4545, 0.4151], grad_fn=<AddBackward0>) torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[0.1641, 0.2789, 0.2659],\n",
      "        [0.4078, 0.2717, 0.2481],\n",
      "        [0.2756, 0.1579, 0.0664],\n",
      "        [0.1429, 0.3883, 0.3981]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([2.3694e-38, 2.3694e-38, 2.3694e-38], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork(4, 3)\n",
    "y = mylinear(x)\n",
    "print (y, y.shape)\n",
    "for param in mylinear.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb6fea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Linear 클래스\n",
    "- 지금까지는 Linear Layer 와 PyTorch 기반 신경망 모듈 구현 방법을 이해하기 위해, Linear Layer 를 PyTorch 의 신경망 모듈 클래스로 구현해본 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00aebadb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1542, 0.2378, 0.4942], grad_fn=<AddBackward0>) torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1641, -0.2789, -0.2659,  0.4078],\n",
      "        [ 0.2717, -0.2481, -0.2756, -0.1579],\n",
      "        [ 0.0664, -0.1429,  0.3883, -0.3981]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1542, 0.2378, 0.4942], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mylinear = nn.Linear(4, 3)\n",
    "y = mylinear(x)\n",
    "print (y, y.shape)\n",
    "\n",
    "for param in mylinear.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fbc80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PyTorch 신경망 모델과 nn.Linear\n",
    "- 하나의 모듈은 내부에 여러 모듈이 있을 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80841f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # super() 함수는 super(subclass 이름, subclass 객체).__init__()  와 같이 써야 하지만,\n",
    "        # 하부클래스 선언 내부에서 super() 호출 시는 super().__init__() 와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "        super().__init__()\n",
    "        # __init__() 에서 신경망 계층 초기화\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim)\n",
    "        # |y| = (input_dim) * (input_dim, output_dim) + (output_dim)\n",
    "        #      = (output_dim)\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0538ea81",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2471, -0.3459,  0.3345], grad_fn=<AddBackward0>) torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[-0.0041, -0.0945,  0.2776,  0.0749],\n",
      "        [ 0.4575, -0.0330, -0.2035,  0.0262],\n",
      "        [ 0.1726, -0.1153,  0.2236, -0.0214]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2471, -0.3459,  0.3345], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mylinear = NeuralNetwork(4, 3)\n",
    "y = mylinear(x)\n",
    "print (y, y.shape)\n",
    "\n",
    "for param in mylinear.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7690550",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
