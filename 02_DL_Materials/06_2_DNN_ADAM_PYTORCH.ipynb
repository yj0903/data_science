{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2e18c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e709186",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Adam optimizer 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5564d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d281b778",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5000, 10) # 10 개의 feature 가 있는 10000 개의 데이터셋\n",
    "y = torch.zeros(5000, 1)  # 10000 개의 데이터에 대한 실제 신경망에서 예측해야 하는 결과값\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 # 1000 epoch 실행 예정\n",
    "minibatch_size = 256 # Mini-batch 사이즈는 256 으로 정하고, 1 epoch 에 10000 개 데이터를 40개의 Mini-batch 로 나누어 40 iteration 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3367be44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = x.size(-1) \n",
    "output_dim = y.size(-1)\n",
    "\n",
    "# 보통 hidden layer 는 출력에 가까울 수록 작아지게 설계하는 것이 일반적임 (더 좋은 성능)\n",
    "model = nn.Sequential (\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 8),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(8, 6),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(6, output_dim)    \n",
    ")\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # SGD, learning rate 필요\n",
    "# optimizer = torch.optim.Adagrad(model.parameters()) # Adagrad, learning rate 필요없음\n",
    "# optimizer = torch.optim.RMSprop(model.parameters()) # RMSprop, learning rate 필요없음\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True) # NAG, learning rate, momentum 둘다 필요 \n",
    "optimizer = torch.optim.Adam(model.parameters()) # Adam, learning rate 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a087a14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1757, 2099,  592,  ..., 3905, 1444, 1343])\n",
      "20 20\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(x.size(0))\n",
    "print (indices)\n",
    "x_batch_list = torch.index_select(x, 0, index=indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언\n",
    "y_batch_list = torch.index_select(y, 0, index=indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언\n",
    "x_batch_list = x_batch_list.split(minibatch_size, dim=0)\n",
    "y_batch_list = y_batch_list.split(minibatch_size, dim=0)\n",
    "print (len(x_batch_list), len(y_batch_list)) # 5000 / 256 = 19.5 정도 되므로, 256 데이터를 가진 19 개의 Mini-batch 와 나머지 데이터를 가진 1 개의 Mini-batch 로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ec8832",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0246, -0.0869,  0.2153, -0.2255,  0.1885,  0.1242,  0.1030,  0.2866,\n",
      "         -0.2280, -0.0579],\n",
      "        [ 0.3065,  0.1502,  0.2547,  0.0811,  0.3132,  0.1216,  0.1609, -0.1017,\n",
      "          0.0560, -0.0987],\n",
      "        [ 0.1730,  0.0634,  0.2695, -0.1839,  0.0355,  0.1041,  0.3002,  0.1397,\n",
      "         -0.1474, -0.0582],\n",
      "        [ 0.3046, -0.2841,  0.1560,  0.2193, -0.1772,  0.1429,  0.2235,  0.0210,\n",
      "         -0.1494,  0.3107],\n",
      "        [ 0.1031, -0.0787,  0.1857,  0.0930,  0.2896, -0.1298,  0.0166,  0.0717,\n",
      "          0.1572,  0.3217],\n",
      "        [-0.0088,  0.1895,  0.1135, -0.3146,  0.2765, -0.2574, -0.3338,  0.2045,\n",
      "          0.1218, -0.1872],\n",
      "        [-0.0813, -0.2113,  0.2586,  0.2524, -0.2527,  0.0379, -0.0134,  0.1420,\n",
      "          0.2611,  0.1207],\n",
      "        [ 0.1146, -0.1971, -0.1579,  0.0080, -0.0277, -0.1706, -0.1711,  0.1806,\n",
      "          0.1402,  0.0957],\n",
      "        [-0.0108,  0.2657,  0.2939,  0.0070,  0.3268,  0.2580,  0.1475, -0.0538,\n",
      "          0.1387, -0.1428],\n",
      "        [ 0.2263,  0.1767,  0.0825, -0.0540,  0.1406,  0.2704, -0.0242,  0.0264,\n",
      "         -0.2084,  0.1654]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1679,  0.2448,  0.0845,  0.0550,  0.1739, -0.2891,  0.1755,  0.0193,\n",
      "         0.1336, -0.1808], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1704, -0.2564, -0.2502, -0.3080, -0.3186, -0.0045,  0.2714, -0.1856,\n",
      "         -0.1633, -0.3417],\n",
      "        [-0.2560,  0.1226, -0.0629,  0.2302, -0.0196,  0.1018,  0.1474, -0.1679,\n",
      "          0.2036,  0.0148],\n",
      "        [-0.1033, -0.2258,  0.0050, -0.2379, -0.0692, -0.0759,  0.0973,  0.1375,\n",
      "          0.0820, -0.2057],\n",
      "        [ 0.2518,  0.0679,  0.1261,  0.1427,  0.2672,  0.0690, -0.1142, -0.0144,\n",
      "          0.3493, -0.2716],\n",
      "        [-0.2879, -0.1123, -0.2805, -0.0915, -0.3022, -0.0656, -0.1465,  0.1351,\n",
      "         -0.2728,  0.2336],\n",
      "        [ 0.1062, -0.2177,  0.0106, -0.3321,  0.0865,  0.2117,  0.1893, -0.2399,\n",
      "          0.1619, -0.1518],\n",
      "        [ 0.2344, -0.1290, -0.2148, -0.1252, -0.1198,  0.2096, -0.0835, -0.3481,\n",
      "         -0.2199,  0.0393],\n",
      "        [-0.2641,  0.1769, -0.1080, -0.1459,  0.0965, -0.0924,  0.1354,  0.2180,\n",
      "          0.3204, -0.0955]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1846, -0.3421,  0.3207, -0.0012, -0.0136,  0.1823,  0.0306, -0.1119],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0138, -0.1390,  0.0653,  0.4099, -0.2608, -0.2047,  0.2640,  0.4005],\n",
      "        [-0.2938, -0.1324,  0.1075, -0.0642,  0.0571,  0.3067, -0.0746, -0.0362],\n",
      "        [-0.0803, -0.0152,  0.3720, -0.4101,  0.0857, -0.1803, -0.1867, -0.1893],\n",
      "        [-0.0090, -0.1085, -0.1574, -0.3569, -0.0228,  0.2393,  0.0366, -0.1354],\n",
      "        [-0.2346,  0.2256, -0.2231, -0.2606, -0.0842,  0.1660,  0.2737,  0.2908],\n",
      "        [ 0.2095,  0.1073, -0.0798, -0.2005,  0.1336,  0.1352, -0.2829, -0.2859]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3596,  0.3082,  0.0344,  0.1090, -0.3570,  0.1756],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0899,  0.0138, -0.4572, -0.3504, -0.4399, -0.2013]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1377], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x.size(0))\n",
    "\n",
    "    x_batch_list = torch.index_select(x, 0, index=indices)\n",
    "    y_batch_list = torch.index_select(y, 0, index=indices)\n",
    "    x_batch_list = x_batch_list.split(minibatch_size, 0)\n",
    "    y_batch_list = y_batch_list.split(minibatch_size, 0)\n",
    "\n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_function(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9d481",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
