{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fba59d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189c0b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 활성화 함수\n",
    "- \\_\\_init_\\_() 함수에서는 모델에서 사용될 모듈(nn.Linear 등) 과 activation function (활성화 함수) 등을 정의함\n",
    "- forward() 함수에서 실행되어야 하는 연산에 활성화 함수도 적용하면 됨\n",
    "- 주요 활성화 함수\n",
    "   - 시그모이드 함수 : nn.Sigmoid()\n",
    "   - ReLU 함수 : nn.ReLU()\n",
    "   - Leaky ReLU 함수 : nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93abdb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 명시적으로 인자명을 써주는 경우도 많이 쓰임\n",
    "        self.linear = nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "        self.activation = nn.Sigmoid() # 시그모이드 함수\n",
    "        # self.activation = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f84698d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(4)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "model = LinearRegressionModel(4, 3)\n",
    "\n",
    "# F.mse_loss() 대신에 nn.MSELoss() 도 동일한 기능을 하며, 주요 클래스가 nn namespace 에 있으므로, nn.MSELoss() 를 사용키로 함\n",
    "loss_function = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba4af7a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea83b7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6109e-06, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.4156,  0.1900,  0.0570,  0.1866],\n",
      "        [ 0.1644,  0.1956, -0.5121, -0.2394],\n",
      "        [ 0.1067,  0.2290, -0.0424, -0.4314]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0180,  0.3916,  0.1052], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad43174",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 다층 레이어 구현\n",
    "> raw level 로 구현해본 후, 좀더 유용한 클래스를 알아보기로 함\n",
    "- input layer -> hidden layer -> output layer 순으로 순차적으로 작성해주면 됨\n",
    "  - 내부 행렬곱 조건만 유의해주면 됨\n",
    "- activation function 적용은 output layer 에는 적용하지 않는 것이 일반적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed843caf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear3 = nn.Linear(10, 10)\n",
    "        self.linear4 = nn.Linear(10, output_dim)        \n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim, output_dim)\n",
    "        hidden = self.activation(self.linear1(x)) # |hidden| = (input_dim, 5)\n",
    "        hidden = self.activation(self.linear2(hidden)) # |hidden| = (5, 5)        \n",
    "        hidden = self.activation(self.linear3(hidden)) # |hidden| = (5, 5)                \n",
    "        y = self.linear4(hidden) # 마지막 출력에는 activation 함수를 사용하지 않는 것이 일반적임\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989dbf8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(4)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1f0d3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c867167c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4134e-10, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2381, -0.1646, -0.4914,  0.1632],\n",
      "        [-0.2515,  0.0371, -0.2751,  0.3417],\n",
      "        [ 0.3883,  0.2373,  0.1406,  0.2542],\n",
      "        [ 0.0055,  0.3668, -0.1642,  0.2487],\n",
      "        [-0.3894,  0.2901,  0.4721,  0.1022],\n",
      "        [-0.0896,  0.3571,  0.4116,  0.1600],\n",
      "        [-0.3992, -0.1270, -0.2315, -0.3204],\n",
      "        [-0.4018,  0.0802, -0.3038,  0.1425],\n",
      "        [-0.2204, -0.3152, -0.0634,  0.2391],\n",
      "        [-0.3342,  0.4009, -0.1703, -0.2847]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3448,  0.4189,  0.2268, -0.2005, -0.4228, -0.2593, -0.4047, -0.0025,\n",
      "         0.4826,  0.2634], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2806, -0.1956, -0.2087,  0.0386,  0.2737,  0.0701,  0.1040, -0.2926,\n",
      "         -0.1926, -0.2716],\n",
      "        [ 0.1223, -0.0471,  0.2707,  0.0010,  0.2944, -0.1528, -0.2384,  0.0343,\n",
      "          0.1140, -0.2458],\n",
      "        [ 0.1451, -0.1123,  0.0841,  0.0708,  0.0214,  0.0293, -0.0468, -0.1758,\n",
      "         -0.0325, -0.0270],\n",
      "        [-0.2452,  0.0910,  0.0846, -0.1138, -0.2302, -0.2500, -0.2752, -0.0721,\n",
      "         -0.2438,  0.0611],\n",
      "        [-0.2061,  0.2354,  0.1095, -0.2908,  0.3142, -0.2241, -0.1098, -0.2222,\n",
      "          0.2807,  0.1337],\n",
      "        [ 0.1272,  0.1498, -0.0154,  0.2272, -0.0051, -0.0419,  0.1840,  0.1891,\n",
      "          0.0966, -0.0887],\n",
      "        [ 0.0493, -0.0784,  0.1154,  0.0619, -0.1235, -0.1779, -0.1186,  0.0272,\n",
      "          0.0622, -0.1772],\n",
      "        [ 0.1636,  0.0101, -0.0942, -0.1188,  0.2781,  0.0274, -0.3146, -0.2234,\n",
      "          0.1126, -0.0380],\n",
      "        [-0.3104,  0.2109, -0.3174, -0.2099, -0.1743,  0.3104, -0.3107, -0.1876,\n",
      "         -0.1551, -0.1842],\n",
      "        [ 0.0228, -0.1028, -0.1404, -0.0553,  0.0610,  0.0806, -0.2240, -0.0069,\n",
      "          0.1225,  0.1074]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0073, -0.3014,  0.0035, -0.1923, -0.0236,  0.3287, -0.0565,  0.1971,\n",
      "         0.1208, -0.0237], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1943,  0.2187,  0.2200,  0.2123,  0.1420,  0.2303, -0.1751,  0.2835,\n",
      "         -0.2643,  0.1970],\n",
      "        [-0.1923,  0.2057, -0.1166, -0.2462,  0.2828, -0.2848,  0.2063, -0.0546,\n",
      "         -0.2970,  0.2077],\n",
      "        [ 0.2970,  0.2338, -0.1907,  0.0125,  0.2652,  0.1745, -0.0627,  0.2647,\n",
      "         -0.1406,  0.2704],\n",
      "        [-0.0341, -0.2212,  0.0090, -0.2376,  0.0253,  0.0086, -0.0209,  0.1734,\n",
      "         -0.0813, -0.0665],\n",
      "        [ 0.1431, -0.0298, -0.1415,  0.1185,  0.0195,  0.2801, -0.1102,  0.1068,\n",
      "          0.2805, -0.0457],\n",
      "        [-0.0193,  0.2818, -0.2672, -0.1087, -0.1916,  0.0645, -0.1194, -0.3071,\n",
      "          0.1978, -0.2639],\n",
      "        [-0.0732, -0.2816, -0.0048, -0.0675, -0.3006,  0.1915,  0.1116, -0.2705,\n",
      "         -0.0150,  0.0509],\n",
      "        [-0.2665, -0.1682, -0.3100, -0.1603, -0.2271, -0.1749,  0.1913,  0.2591,\n",
      "          0.2712,  0.0425],\n",
      "        [ 0.2930,  0.1063, -0.1455, -0.2201, -0.0819, -0.0695, -0.0787, -0.0872,\n",
      "          0.0625,  0.2831],\n",
      "        [-0.1445, -0.1198, -0.0671, -0.0125, -0.1218, -0.2960, -0.1785,  0.1493,\n",
      "         -0.0562,  0.1856]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0784,  0.1730,  0.1770, -0.0587, -0.1057,  0.0749, -0.3119, -0.1442,\n",
      "         0.2589,  0.2313], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3093, -0.1765,  0.3175, -0.0210, -0.0544, -0.0816,  0.0639, -0.0976,\n",
      "         -0.2180, -0.0400],\n",
      "        [-0.1333,  0.0231,  0.1424,  0.2628,  0.0437, -0.0828, -0.0559,  0.3161,\n",
      "         -0.1743, -0.1717],\n",
      "        [-0.2809,  0.0896, -0.2414,  0.0305, -0.1374, -0.1702,  0.2472,  0.2651,\n",
      "          0.2829, -0.1942]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0894,  0.0548,  0.1058], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9def38a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Sequential\n",
    "- nn.Sequential 은 순서를 갖는 모듈의 컨테이너를 의미함\n",
    "- 순차적으로 연산되는 레이어만 있을 경우에는, nn.Sequential 을 통해 순서대로 각 레이어를 작성하면 그대로 실행됨\n",
    "  - 중간에 activation function 이 적용된다면, activation function 도 순서에 맞게 넣어주면 자동 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e8bcb94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(4)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "\n",
    "input_dim = x.size(0)\n",
    "output_dim = y.size(0)\n",
    "\n",
    "model = nn.Sequential (\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, output_dim)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e77be071",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4440e-10, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.2754, -0.0372, -0.2407,  0.3626],\n",
      "        [ 0.4701,  0.0925,  0.0651, -0.3780],\n",
      "        [-0.0094, -0.2228, -0.0772, -0.2660],\n",
      "        [-0.1651,  0.3736, -0.0696,  0.3334],\n",
      "        [ 0.0909, -0.2044, -0.2411,  0.4714],\n",
      "        [-0.2170,  0.1812, -0.4926, -0.4065],\n",
      "        [-0.2669, -0.4894, -0.2360, -0.2656],\n",
      "        [-0.3523,  0.4542,  0.2489,  0.0503],\n",
      "        [-0.3685,  0.3352, -0.4547, -0.1881],\n",
      "        [-0.2065, -0.2266, -0.4444,  0.4920]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0641, -0.0401,  0.2730, -0.3222,  0.4448, -0.4015,  0.1903,  0.4497,\n",
      "         0.3051, -0.1508], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2571,  0.3119,  0.3030,  0.2072, -0.1458,  0.1450,  0.0371,  0.2615,\n",
      "          0.0027,  0.0714],\n",
      "        [ 0.2061,  0.2504, -0.0309,  0.1212,  0.1219, -0.2075,  0.2148, -0.0964,\n",
      "         -0.2055, -0.0731],\n",
      "        [ 0.0305, -0.1485,  0.2762, -0.1629, -0.2989,  0.1144, -0.1850, -0.0025,\n",
      "          0.0460, -0.2435],\n",
      "        [ 0.2134, -0.1313, -0.0215, -0.2248,  0.0433, -0.2826,  0.0970,  0.0078,\n",
      "         -0.1221, -0.1891],\n",
      "        [ 0.1155,  0.1329, -0.2543, -0.2491,  0.1967,  0.1238,  0.0010,  0.1864,\n",
      "         -0.1380, -0.0492],\n",
      "        [ 0.0595, -0.2350,  0.0908, -0.1263,  0.0733,  0.1120,  0.1788, -0.2140,\n",
      "          0.1393, -0.0411],\n",
      "        [ 0.0073,  0.1260, -0.0982, -0.2181, -0.0022, -0.0308,  0.2876,  0.2984,\n",
      "         -0.2951,  0.1935],\n",
      "        [ 0.2265,  0.1335, -0.0616, -0.0216,  0.2267, -0.0917,  0.2017, -0.2965,\n",
      "          0.2277, -0.2442],\n",
      "        [-0.1382,  0.2552,  0.2388,  0.1139, -0.3141, -0.0935,  0.0805,  0.2571,\n",
      "         -0.2817,  0.0828],\n",
      "        [-0.2997, -0.0587, -0.2084, -0.1742, -0.1876,  0.1565, -0.2181, -0.2256,\n",
      "         -0.3066,  0.2780]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0705, -0.1944,  0.0763, -0.2118, -0.0736, -0.2243, -0.2397, -0.2948,\n",
      "         0.2443, -0.0913], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1308, -0.0590,  0.2883,  0.0057,  0.2446,  0.0183,  0.0762,  0.3050,\n",
      "          0.1063,  0.3178],\n",
      "        [-0.1509, -0.2068, -0.3136,  0.2892, -0.2376, -0.2995,  0.0109, -0.1190,\n",
      "          0.0694,  0.0656],\n",
      "        [-0.2320, -0.1058, -0.1445, -0.2811,  0.2646,  0.1251, -0.0126, -0.1571,\n",
      "          0.0916, -0.2455],\n",
      "        [ 0.1622, -0.1396,  0.2031, -0.1324,  0.2944, -0.2198, -0.2959,  0.2398,\n",
      "          0.0976, -0.2190],\n",
      "        [-0.2252,  0.1343, -0.0406, -0.1906,  0.3099, -0.2092, -0.2208,  0.0643,\n",
      "         -0.1954,  0.2897],\n",
      "        [-0.2220, -0.0525, -0.2099, -0.1363,  0.1770, -0.3151,  0.1362,  0.2760,\n",
      "          0.0024, -0.0252],\n",
      "        [ 0.0772, -0.3143, -0.2058, -0.1066,  0.2697, -0.3186, -0.2676,  0.2262,\n",
      "         -0.1771, -0.0131],\n",
      "        [ 0.1280, -0.2589, -0.1697,  0.1825,  0.3011,  0.1380, -0.2145, -0.1018,\n",
      "         -0.0880, -0.0092],\n",
      "        [-0.1560, -0.2543, -0.1518, -0.0500,  0.1094,  0.0904,  0.1143,  0.2624,\n",
      "          0.1760,  0.0423],\n",
      "        [ 0.1750,  0.0846,  0.2866, -0.2258,  0.1816,  0.0248,  0.1620, -0.1452,\n",
      "         -0.1742, -0.1020]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2719, -0.2234, -0.0100,  0.0264, -0.0245, -0.0801,  0.2768,  0.2113,\n",
      "         0.1785, -0.2809], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0776, -0.0420,  0.1444,  0.0120,  0.3093, -0.2157, -0.3271,  0.1800,\n",
      "         -0.3094, -0.0907],\n",
      "        [ 0.1982,  0.2860, -0.0690, -0.2202, -0.0136,  0.1063,  0.0682, -0.1621,\n",
      "         -0.1199,  0.2019],\n",
      "        [ 0.0478,  0.3208,  0.0413, -0.0091, -0.2193, -0.2457, -0.3098,  0.2183,\n",
      "         -0.1489, -0.3027]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1337, 0.0311, 0.0473], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c8860",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD 방식 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2006ba5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 랜덤하게 데이터를 섞기 위한 함수\n",
    "  - torch.randperm(n) : 0 ~ n - 1 까지의 정수를 랜덤하게 섞어서, 순열(배열)을 만들어 줌\n",
    "  - torch.index_select(텐서객체, 차원번호, 인덱스텐서)\n",
    "    - 차원번호는 예를 들어, |x| = (3, 4) 에서 0 차원에 해당하는 값은 3 (행으로 이해하면 됨), 1 차원에 해당하는 값은 4 (열로 이해하면 됨)\n",
    "  - 특정 차원의 나열된 인덱스 번호 순서대로, 데이터를 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23fae81c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2001,  0.0867,  0.6732, -0.7364],\n",
      "        [-0.5086, -0.7707, -0.5032,  0.4447],\n",
      "        [ 0.7114, -0.3392, -0.3495, -0.4505]])\n",
      "tensor([1, 2])\n",
      "tensor([[-0.5086, -0.7707, -0.5032,  0.4447],\n",
      "        [ 0.7114, -0.3392, -0.3495, -0.4505]])\n",
      "tensor([[ 0.0867,  0.6732],\n",
      "        [-0.7707, -0.5032],\n",
      "        [-0.3392, -0.3495]])\n"
     ]
    }
   ],
   "source": [
    "data1 = torch.randn(3, 4)\n",
    "print (data1)\n",
    "indices = torch.tensor([1, 2])\n",
    "print (indices)\n",
    "print (torch.index_select(data1, 0, indices))\n",
    "print (torch.index_select(data1, 1, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009031f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 테스트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcfe9e9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5000, 10) # 10 개의 feature 가 있는 10000 개의 데이터셋\n",
    "y = torch.zeros(5000, 1)  # 10000 개의 데이터에 대한 실제 신경망에서 예측해야 하는 결과값\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 # 1000 epoch 실행 예정\n",
    "minibatch_size = 256 # Mini-batch 사이즈는 256 으로 정하고, 1 epoch 에 10000 개 데이터를 40개의 Mini-batch 로 나누어 40 iteration 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d65142",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = x.size(-1) \n",
    "output_dim = y.size(-1)\n",
    "\n",
    "# 보통 hidden layer 는 출력에 가까울 수록 작아지게 설계하는 것이 일반적임 (더 좋은 성능)\n",
    "model = nn.Sequential (\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 8),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(8, 6),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(6, output_dim)    \n",
    ")\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4bbef29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 583,  781, 2536,  ..., 3404, 4193, 3028])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(x.size(0))\n",
    "print (indices)\n",
    "x_batch_list = torch.index_select(x, 0, index=indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언\n",
    "y_batch_list = torch.index_select(y, 0, index=indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c988411e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ece0f2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "x_batch_list = x_batch_list.split(minibatch_size, dim=0)\n",
    "y_batch_list = y_batch_list.split(minibatch_size, dim=0)\n",
    "print (len(x_batch_list), len(y_batch_list)) # 5000 / 256 = 19.5 정도 되므로, 256 데이터를 가진 19 개의 Mini-batch 와 나머지 데이터를 가진 1 개의 Mini-batch 로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41810e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 참고: 파이썬 zip() 내장 함수\n",
    "  - for 구문에서, 두 개 이상의 리스트 변수를 같은 인덱스 번호의 데이터 별로 묶어서, 튜플로 반환할 때 많이 활용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "217db894",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1684e-15, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1279,  0.1960,  0.3121, -0.0354, -0.0729, -0.1457,  0.1145, -0.1305,\n",
      "          0.2323,  0.0436],\n",
      "        [-0.2136, -0.1330,  0.0991,  0.2277,  0.1681,  0.0313, -0.2268, -0.2854,\n",
      "          0.2656, -0.0918],\n",
      "        [ 0.3094, -0.0944,  0.0782, -0.0658, -0.0181,  0.1384, -0.1609, -0.2764,\n",
      "          0.2483, -0.0946],\n",
      "        [-0.1138, -0.2362, -0.0691, -0.0637, -0.2234, -0.2830, -0.1459,  0.1672,\n",
      "         -0.2570, -0.1796],\n",
      "        [ 0.0538,  0.0277,  0.3042, -0.2546,  0.1551, -0.0498, -0.1253,  0.1937,\n",
      "         -0.1640,  0.0430],\n",
      "        [-0.1426,  0.3114, -0.2172,  0.0948,  0.0868,  0.1932,  0.2807,  0.1693,\n",
      "         -0.2176, -0.1785],\n",
      "        [ 0.2155, -0.1413,  0.0165, -0.1992,  0.1384, -0.2318, -0.0168, -0.0837,\n",
      "         -0.0724,  0.1680],\n",
      "        [-0.2190,  0.2403, -0.2882,  0.0983, -0.1119,  0.1632, -0.0787,  0.3036,\n",
      "         -0.2573,  0.2525],\n",
      "        [ 0.0003,  0.0127,  0.2067,  0.2793,  0.2224,  0.2250,  0.1780,  0.2682,\n",
      "          0.2827,  0.2918],\n",
      "        [ 0.2871, -0.2444,  0.1688, -0.1169,  0.1866,  0.1056, -0.1123,  0.2346,\n",
      "          0.0152,  0.2552]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0627, -0.2952, -0.2863,  0.0283,  0.3118,  0.1728,  0.0190,  0.0246,\n",
      "        -0.2796, -0.1920], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0596,  0.2328,  0.1380, -0.0934,  0.0414,  0.1291,  0.0822,  0.2264,\n",
      "          0.1423, -0.1618],\n",
      "        [ 0.2563, -0.2669, -0.0225,  0.1803,  0.1237,  0.2625, -0.2774,  0.1287,\n",
      "         -0.1880,  0.1945],\n",
      "        [-0.1534,  0.0079,  0.2145, -0.3103,  0.1016, -0.2568,  0.1155,  0.2010,\n",
      "         -0.2810,  0.1680],\n",
      "        [-0.1696, -0.0230,  0.0015, -0.2237,  0.1487, -0.2757,  0.2192,  0.2684,\n",
      "          0.0792,  0.1131],\n",
      "        [-0.0466, -0.1411,  0.2405,  0.1319,  0.1582, -0.0068,  0.0931,  0.0872,\n",
      "          0.1032, -0.2719],\n",
      "        [ 0.2520,  0.0297,  0.0571,  0.1405, -0.3020, -0.0954, -0.2318, -0.1308,\n",
      "         -0.2704,  0.2896],\n",
      "        [ 0.0003,  0.2392,  0.0906, -0.2984,  0.1192,  0.2990, -0.0321,  0.1824,\n",
      "         -0.1670, -0.3116],\n",
      "        [-0.2986,  0.1479,  0.2874,  0.0200,  0.2193, -0.1822, -0.2191, -0.0182,\n",
      "          0.2905, -0.0587]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0588,  0.2475,  0.0089, -0.0973, -0.1174, -0.2916,  0.1624, -0.0070],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2874, -0.1785,  0.0845,  0.3465,  0.2538,  0.0598,  0.0344,  0.2363],\n",
      "        [-0.2271, -0.2140,  0.2309,  0.0638,  0.0310,  0.0633,  0.3514,  0.1176],\n",
      "        [-0.0461,  0.1049,  0.0775, -0.2355,  0.2935, -0.3006,  0.2627,  0.1516],\n",
      "        [ 0.0252,  0.0680,  0.1237, -0.0080, -0.2804,  0.2734,  0.0019,  0.2168],\n",
      "        [-0.1209,  0.0751,  0.3074, -0.2069, -0.3493,  0.1760,  0.3318, -0.1441],\n",
      "        [-0.0018,  0.1421, -0.2652, -0.3277, -0.2833, -0.0841, -0.2014,  0.1610]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1980, -0.1104, -0.0068, -0.0857, -0.1999,  0.1079],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1285,  0.3703, -0.2263, -0.3732,  0.1727,  0.2607]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0271], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x.size(0))\n",
    "\n",
    "    x_batch_list = torch.index_select(x, 0, index=indices)\n",
    "    y_batch_list = torch.index_select(y, 0, index=indices)\n",
    "    x_batch_list = x_batch_list.split(minibatch_size, 0)\n",
    "    y_batch_list = y_batch_list.split(minibatch_size, 0)\n",
    "\n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_function(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fa230",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
