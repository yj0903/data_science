{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iRO3WmbqoDE"
   },
   "source": [
    "### CIFAR10 데이터셋\n",
    "- ‘비행기(airplane)’, ‘자동차(automobile)’, ‘새(bird)’, ‘고양이(cat)’, ‘사슴(deer)’, ‘개(dog)’, ‘개구리(frog)’, ‘말(horse)’, ‘배(ship)’, ‘트럭(truck)’ 개의 3채널(컬러), 32x32 이미지와 레이블로 구성 (60000개) \n",
    "- https://huggingface.co/datasets/cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj-Xysbzq_9f"
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_7rNZfIf301"
   },
   "source": [
    "pip install -q 옵션: 더 적은 출력표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MCTZFkFw6i6"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.22.1 datasets==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEXPJ1rTtIUZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZXEXiozxB-4"
   },
   "source": [
    "## Loading the data\n",
    "- datasets.load_dataset(데이터셋이름, split=['train[:x]', 'test[:y]'])\n",
    "  - 전체 데이터셋 사이즈 중, train 데이터에서 5000까지, test 데이터에서 2000개까지 가져옴\n",
    "    - 해당 데이터에 train 명으로 분리된 데이터셋과 test 명으로 분리된 데이터셋이 이미 존재함\n",
    "  - train_test_split() 을 사용하여, validiation set 도 구성 가능\n",
    "    - train 과 test 키로 각 데이터셋이 구성됨\n",
    "- https://huggingface.co/datasets/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "50027063126f4b9eb954487b6846a672",
      "81af217059554ee79481146c77ef75fb",
      "0630a1ab3d45430cae2204f9397ca248",
      "d7b8c9690e4f450ebdacf442bb087003",
      "4fe81a4dd7fa4dd38f17c0aa01008dfc",
      "708828e296ff4f00aa03981ff9833f17",
      "1bd57cfb57ec4fb694a7b1c92d241a93",
      "62c524e7509f4cd3b87dc850035d3caa",
      "934586d5c99648d6bc64a19f8e3a6837",
      "23264b1b375f42adba8c980a68063e78",
      "54e1f611ea7241a6bdaf7b4a2231b265"
     ]
    },
    "id": "XC9HqG5u750_",
    "outputId": "8456c5ce-218a-4974-c768-09f1a8085491"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50027063126f4b9eb954487b6846a672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['img', 'label'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywJ0iarBhXpc"
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- Vision Transformer 는 동일 이미지 사이즈와 동일 채널별 Normalization 시에 성능이 좋음\n",
    "- ViTFeatureExtractor() 를 통해, 해당 Pre-Trained 모델의 학습시 적용된 config 을 확인할 수 있음\n",
    "- 채널별 픽셀값은 'pixel_values', 해당 이미지의 분류값은 'labels' 에 넣어주면, 해당 Pre-Trained 모델로 학습 및 예측 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik1n1OArMTk4",
    "outputId": "68964497-816e-4c73-964c-87fff4aaf097"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/preprocessor_config.json\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"size\": 224\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "# vit 모델: https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWoWAgYzZAe"
   },
   "source": [
    "### Augmentation\n",
    "- 적은 데이터를 증강하는 기법으로 이미지 모델에서 성능을 높이는데 기여한 기법\n",
    "- 또한 다양한 test 데이터에 대해서도 성능을 낼 수 있도록, 데이터를 임의로 다양하게 변형하여, 학습시키기 위해서도 많이 사용함\n",
    "- pytorch torchvision 에서 제공하는 데이터셋은 데이터 변경을 용이하게 할 수 있도록 몇 가지 변형을 제공함\n",
    "- 주요 함수\n",
    "   - torchvision.transforms.ToTensor() : PIL 이미지 또는 ndarray 데이터를 텐서 형태로 변형시켜줌\n",
    "   - torchvision.transforms.Normalize(mean, std)\n",
    "      - mean, std 는 각 채널별 평균과 표준편차 (데이터 정규화를 위한 기법), **텐서에만 적용 가능**\n",
    "      - 예: 3 채널 데이터라면,\n",
    "         - 각 채널의 평균, 표준편차를 0.5 로 셋한다면, \n",
    "         - transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "   - torchvision.transforms.Resize(size)\n",
    "      - 이미지의 사이즈 변경\n",
    "      - 예: 이미지를 224 x 224 로 변경하고자 한다면,\n",
    "         - transforms.Resize((224, 224))\n",
    "   - torchvision.transforms.Compose()\n",
    "      - 여러 transform 을 하나로 구성하는 기능\n",
    "   - https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8fcwD0OxEHP",
    "outputId": "efd5b82f-3634-4158-f3da-5e46eab46e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'label', 'pixel_values'])\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'int'> 5\n",
      "<class 'torch.Tensor'> torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "transforms_for_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(feature_extractor.size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transforms_for_val = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(feature_extractor.size),\n",
    "            transforms.CenterCrop(feature_extractor.size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(imagedata):\n",
    "    # 파이썬 Comprehension 참고: https://www.fun-coding.org/PL&OOP5-2.html\n",
    "    imagedata['pixel_values'] = [transforms_for_train(image.convert(\"RGB\")) for image in imagedata['img']]\n",
    "    return imagedata\n",
    "\n",
    "def test_transforms(imagedata):\n",
    "    # 파이썬 Comprehension 참고: https://www.fun-coding.org/PL&OOP5-2.html\n",
    "    imagedata['pixel_values'] = [transforms_for_val(image.convert(\"RGB\")) for image in imagedata['img']]\n",
    "    return imagedata\n",
    "\n",
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(test_transforms)\n",
    "test_ds.set_transform(test_transforms)\n",
    "\n",
    "print (train_ds[0].keys())\n",
    "print (type(train_ds[0]['img']))\n",
    "print (type(train_ds[0]['label']), train_ds[0]['label'])\n",
    "print (type(train_ds[0]['pixel_values']), train_ds[0]['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNnLHRNbzsWU"
   },
   "source": [
    "### Trainer 활용을 위해 필요한 data_collator 함수 \n",
    "  - 인덱스 번호 기반 데이터셋(map-style dadaset) 을 기반으로 mini-batch 구성시 샘플을 리스트로 합쳐주는 기능을 구현해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMOC0oBnhjRM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(imagedata):\n",
    "    # 파이썬 Comprehension 참고: https://www.fun-coding.org/PL&OOP5-2.html\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in imagedata])\n",
    "    labels = torch.tensor([example[\"label\"] for example in imagedata])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# DataLoader 사용시에는 다음과 같이 사용할 수 있음\n",
    "# train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDefWdsUxEsK"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "- https://huggingface.co/google/vit-base-patch16-224-in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZSYo7nUhtao",
    "outputId": "1f3c7de0-eeba-45e7-9585-ed92f97404a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'automobile',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
    "label2id = {label:id for id, label in id2label.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bgO-lD1pKIJ",
    "outputId": "86ae26dc-7ce5-493a-9d7b-804045c2b194"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECs926Tn2Q0M",
    "outputId": "395344d4-66e7-438b-f0e6-3ad4c46893f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": 0,\n",
      "    \"automobile\": 1,\n",
      "    \"bird\": 2,\n",
      "    \"cat\": 3,\n",
      "    \"deer\": 4,\n",
      "    \"dog\": 5,\n",
      "    \"frog\": 6,\n",
      "    \"horse\": 7,\n",
      "    \"ship\": 8,\n",
      "    \"truck\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.22.1\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                  num_labels=10,\n",
    "                                                  id2label=id2label,\n",
    "                                                  label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "difJEMv7a9Sr",
    "outputId": "c0759b2b-7547-486a-84e9-3febc6fbb46e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
       "  \"architectures\": [\n",
       "    \"ViTModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"airplane\",\n",
       "    \"1\": \"automobile\",\n",
       "    \"2\": \"bird\",\n",
       "    \"3\": \"cat\",\n",
       "    \"4\": \"deer\",\n",
       "    \"5\": \"dog\",\n",
       "    \"6\": \"frog\",\n",
       "    \"7\": \"horse\",\n",
       "    \"8\": \"ship\",\n",
       "    \"9\": \"truck\"\n",
       "  },\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"airplane\": 0,\n",
       "    \"automobile\": 1,\n",
       "    \"bird\": 2,\n",
       "    \"cat\": 3,\n",
       "    \"deer\": 4,\n",
       "    \"dog\": 5,\n",
       "    \"frog\": 6,\n",
       "    \"horse\": 7,\n",
       "    \"ship\": 8,\n",
       "    \"truck\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.22.1\"\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DItK3jKBt4fQ"
   },
   "source": [
    "### Trainer 실행을 위해 필요한 아규먼트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wTw4RXe73vA",
    "outputId": "9e382fda-e74a-4178-892c-aac577f67b1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test-cifar-10\", # 모델 예측과 체크포인트가 저장되는 폴더명 (모델마다 임의 폴더명으로 작성하면 됨)\n",
    "    save_strategy=\"epoch\", # epoch 마다 모델 학습 전략\n",
    "    evaluation_strategy=\"epoch\", # evalution 시, epoch 마다 모델 학습 전략\n",
    "    learning_rate=2e-5, # learning rate\n",
    "    per_device_train_batch_size=16, # CPU/GPU 당 mini-batch 사이즈 \n",
    "    per_device_eval_batch_size=16, # evaluation 시, CPU/GPU 당 mini-batch 사이즈\n",
    "    num_train_epochs=10, # total epoch num\n",
    "    weight_decay=0.01, # optimizer 에 들어갈 weight decay\n",
    "    load_best_model_at_end=True, # 학습 종료시 자동으로 베스트 모델을 로드함\n",
    "    metric_for_best_model=\"accuracy\", # 베스트 모델 측정을 위한 매트릭 (정확도)\n",
    "    logging_dir='logs', # Tensorboard 를 위한 logs 를 저장할 폴더명\n",
    "    remove_unused_columns=False, # 자동으로 모델에서 쓰지 않는 컬럼 삭제 여부\n",
    "    optim=\"adamw_torch\", # 최근 변경(pytorch 에서 제공하는 AdamW optimizer 사용)\n",
    "    lr_scheduler_type=\"constant\", # learning rate scheduler (디폴트: linear)\n",
    "    save_total_limit=10 # 저장할 checkpoints 최대 갯수 (저장용량 초과 에러 방지) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1TIPVs1uA8P"
   },
   "source": [
    "### Metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l2MWwHUEndp"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg8BiyyAuD3r"
   },
   "source": [
    "### Trainer 정의 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RH03LdhY8bkp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IwqvsF4x8mYA",
    "outputId": "07176b6f-0256-4bb7-d189-5e82deaac066"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2820' max='2820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2820/2820 31:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614396</td>\n",
       "      <td>0.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>0.289129</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>0.194291</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.168110</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.179907</td>\n",
       "      <td>0.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.192630</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.191030</td>\n",
       "      <td>0.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.201624</td>\n",
       "      <td>0.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.213412</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-282\n",
      "Configuration saved in test-cifar-10/checkpoint-282/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-282/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-282/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-564\n",
      "Configuration saved in test-cifar-10/checkpoint-564/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-564/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-564/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-846\n",
      "Configuration saved in test-cifar-10/checkpoint-846/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-846/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-846/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-1128\n",
      "Configuration saved in test-cifar-10/checkpoint-1128/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-1128/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-1128/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-1410\n",
      "Configuration saved in test-cifar-10/checkpoint-1410/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-1410/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-1410/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-1692\n",
      "Configuration saved in test-cifar-10/checkpoint-1692/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-1692/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-1692/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-1974\n",
      "Configuration saved in test-cifar-10/checkpoint-1974/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-1974/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-1974/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-2256\n",
      "Configuration saved in test-cifar-10/checkpoint-2256/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-2256/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-2256/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-2538\n",
      "Configuration saved in test-cifar-10/checkpoint-2538/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-2538/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-2538/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-cifar-10/checkpoint-2820\n",
      "Configuration saved in test-cifar-10/checkpoint-2820/config.json\n",
      "Model weights saved in test-cifar-10/checkpoint-2820/pytorch_model.bin\n",
      "Feature extractor saved in test-cifar-10/checkpoint-2820/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test-cifar-10/checkpoint-846 (score: 0.972).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2820, training_loss=0.20501939345758857, metrics={'train_runtime': 1913.4984, 'train_samples_per_second': 23.517, 'train_steps_per_second': 1.474, 'total_flos': 3.48738956568576e+18, 'train_loss': 0.20501939345758857, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWXvWiB-srBC"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ZLv_xdYssuGO",
    "outputId": "1edb9842-1cbd-4fd1-c80c-bdebc3b3e5bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymxkjZc0tBrn",
    "outputId": "bad79289-7cd0-4c8b-8034-7ff35291aae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.19178876280784607, 'test_accuracy': 0.97, 'test_runtime': 30.8608, 'test_samples_per_second': 64.807, 'test_steps_per_second': 4.05}\n"
     ]
    }
   ],
   "source": [
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lz6oy98fwJn2"
   },
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #1976D2;background-color:#E3F2FD;padding:5px;font-size:0.9em;\">\n",
    "본 자료는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 자료를 외부에 공개하지 말아주세요.<br>\n",
    "<b><a href=\"https://school.fun-coding.org/\">잔재미코딩 (https://school.fun-coding.org/)</a> 에서 본 강의를 포함하는 최적화된 로드맵도 확인하실 수 있습니다</b></div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0630a1ab3d45430cae2204f9397ca248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62c524e7509f4cd3b87dc850035d3caa",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_934586d5c99648d6bc64a19f8e3a6837",
      "value": 2
     }
    },
    "1bd57cfb57ec4fb694a7b1c92d241a93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23264b1b375f42adba8c980a68063e78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fe81a4dd7fa4dd38f17c0aa01008dfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50027063126f4b9eb954487b6846a672": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81af217059554ee79481146c77ef75fb",
       "IPY_MODEL_0630a1ab3d45430cae2204f9397ca248",
       "IPY_MODEL_d7b8c9690e4f450ebdacf442bb087003"
      ],
      "layout": "IPY_MODEL_4fe81a4dd7fa4dd38f17c0aa01008dfc"
     }
    },
    "54e1f611ea7241a6bdaf7b4a2231b265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62c524e7509f4cd3b87dc850035d3caa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "708828e296ff4f00aa03981ff9833f17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81af217059554ee79481146c77ef75fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_708828e296ff4f00aa03981ff9833f17",
      "placeholder": "​",
      "style": "IPY_MODEL_1bd57cfb57ec4fb694a7b1c92d241a93",
      "value": "100%"
     }
    },
    "934586d5c99648d6bc64a19f8e3a6837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d7b8c9690e4f450ebdacf442bb087003": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23264b1b375f42adba8c980a68063e78",
      "placeholder": "​",
      "style": "IPY_MODEL_54e1f611ea7241a6bdaf7b4a2231b265",
      "value": " 2/2 [00:00&lt;00:00, 41.05it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
